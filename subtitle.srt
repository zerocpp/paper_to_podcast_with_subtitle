1
00:00:00,000 --> 00:00:05,940
Hey everyone, ready for a deep dive? Let's talk about AI, specifically LLMs, those large language

2
00:00:05,940 --> 00:00:10,760
models. We've got a real interesting research paper hot off the press. Title's a bit of a

3
00:00:10,760 --> 00:00:17,480
mouthful though, ready? LLMs know more than they show on the intrinsic representation of LLM

4
00:00:17,480 --> 00:00:22,680
hallucinations. Kitchy, right. But it gets straight to the point, doesn't it? Why do these powerful

5
00:00:22,680 --> 00:00:28,200
LLMs sometimes just make stuff up? That's what we're diving into today. It's like when you ask

6
00:00:28,200 --> 00:00:32,140
an LLM for a simple fact, you know, for work or something, and it gives you something totally

7
00:00:32,140 --> 00:00:36,800
off base, you're just left there thinking, did it? Did it just make that up? Exactly. And this

8
00:00:36,800 --> 00:00:40,860
research team, they didn't just stop at, oops, the AI got it wrong. They actually looked inside

9
00:00:40,860 --> 00:00:45,780
several different LLMs, wanted to see if there was something more going on, something about how these

10
00:00:45,780 --> 00:00:51,540
models understand or maybe misunderstand the whole idea of truth. One thing that really caught my eye

11
00:00:51,540 --> 00:00:57,140
was this whole exact answer token thing. I know we've got some tech savvy listeners, but just in

12
00:00:57,140 --> 00:01:02,940
case, you mind breaking that down for us. Sure, sure. Think of LLMs as being made of these, well,

13
00:01:03,120 --> 00:01:07,980
building blocks of language. They're called tokens, kind of like how we use words, but for these

14
00:01:07,980 --> 00:01:12,560
models. Now, what they found is that some tokens, when the LLM is putting together its response,

15
00:01:12,860 --> 00:01:19,380
they matter more when it comes to whether the model think what it's saying is true. Those are the exact

16
00:01:19,380 --> 00:01:25,340
answer tokens. So it's not just the final answer itself, but these specific tokens inside it that give

17
00:01:25,340 --> 00:01:30,640
us clues about what the LLM is thinking, right? Like, is it sure of itself or is it about to go off the

18
00:01:30,640 --> 00:01:35,960
rails? Exactly. And that's a big deal because you see, a lot of the time people just looked at the

19
00:01:35,960 --> 00:01:42,680
final output, the last token to understand errors. This research says, hold on, there's more to it.

20
00:01:43,060 --> 00:01:47,900
These exact answer tokens, often right in the middle of the response, they're like a much stronger

21
00:01:47,900 --> 00:01:52,780
signal about how confident the model is. It's almost like the LLM is like emphasizing,

22
00:01:52,780 --> 00:01:57,620
maybe even second guessing, the most important part of its answer. Makes you wonder if we could

23
00:01:57,620 --> 00:02:01,160
use this to predict when an LLM is about to have one of those hallucination moments.

24
00:02:01,500 --> 00:02:05,400
That's what the researchers wanted to know. They actually built these little AI detectives,

25
00:02:05,560 --> 00:02:12,260
probing classifiers, to look at the LLM's internal signals at the exact moment it's processing these

26
00:02:12,260 --> 00:02:18,400
exact answer tokens, like trying to read the AI's mind. Okay, hold on. So they built AIs to spy on

27
00:02:18,400 --> 00:02:23,740
other AIs. Wild. But did it work? Could they tell if an LLM was about to give a wrong answer

28
00:02:23,740 --> 00:02:28,300
just by looking at those tokens? It's pretty amazing. But yeah, they were surprisingly accurate.

29
00:02:28,720 --> 00:02:32,380
Way better at predicting those errors than older methods that focused on other things.

30
00:02:32,520 --> 00:02:36,720
So we're talking about a potential breakthrough here, right? By looking at how much emphasis an

31
00:02:36,720 --> 00:02:42,240
LLM puts on certain words, we might be able to tell if we should trust what it's saying. That's huge

32
00:02:42,240 --> 00:02:47,060
for making AI more reliable. It really is. But there's always a but, isn't there? They didn't

33
00:02:47,060 --> 00:02:51,520
stop there. The researchers wanted to know if these AI detectives, these probing classifiers,

34
00:02:51,620 --> 00:02:56,540
could generalize. Could they spot errors across different types of tasks? So if they trained a

35
00:02:56,540 --> 00:03:03,280
probe on, say, trivia questions, would it also catch errors in, I don't know, analyzing a movie review?

36
00:03:03,660 --> 00:03:08,320
Exactly. And this is where things get even more interesting. While the probes did show some

37
00:03:08,320 --> 00:03:15,280
ability to transfer their error spotting skills, it wasn't a perfect system. It seems like different

38
00:03:15,280 --> 00:03:21,160
AI skills, like knowing a fact versus understanding how someone feels, those might rely on different

39
00:03:21,160 --> 00:03:27,820
mechanisms for figuring out truth. So no single lie detector for LLMs, then? Makes sense, I guess.

40
00:03:27,940 --> 00:03:32,400
Our brains are complicated, so why wouldn't these models be the same? But if that's the case,

41
00:03:32,480 --> 00:03:36,640
how do we even begin to understand these different errors, let alone fix them?

42
00:03:36,640 --> 00:03:40,960
Well, the researchers, they really dug into this. They focused on one type of task,

43
00:03:41,660 --> 00:03:47,140
answering trivia questions. But instead of just right or wrong, they came up with this whole system

44
00:03:47,140 --> 00:03:51,240
to categorize errors based on how often the LLM messed up the same question.

45
00:03:51,780 --> 00:03:55,720
So they were looking for patterns, trying to understand the AI's thought process. Like,

46
00:03:55,800 --> 00:03:58,340
okay, sometimes you get this right, sometimes wrong. What's the deal?

47
00:03:58,700 --> 00:04:03,060
Exactly. And they found some interesting things. For example, sometimes the LLM would mostly get an

48
00:04:03,060 --> 00:04:09,040
answer wrong, but then, bam, nail it out of nowhere. That little difference, always wrong versus

49
00:04:09,040 --> 00:04:13,180
sometimes right, could mean completely different things are causing those errors.

50
00:04:13,180 --> 00:04:19,600
That IS wild. Like, the LLM knows the answer somewhere in there, but can't always access it.

51
00:04:19,660 --> 00:04:25,140
It's like when you're taking a test and you know you studied it, but it's just not there. But could the

52
00:04:25,140 --> 00:04:30,720
researchers predict what kind of mistake the LLM would make just by looking at those signals inside?

53
00:04:30,720 --> 00:04:33,720
That just seems next level. You're not going to believe this. They gave it a shot.

54
00:04:33,860 --> 00:04:38,360
Yeah. And the results were pretty impressive. Those error types, those different ways an LLM

55
00:04:38,360 --> 00:04:43,400
can trip up, they are somewhat predictable just by, you know, looking at the model's internal signals.

56
00:04:43,640 --> 00:04:47,720
So we're not just spotting errors anymore. We're figuring out the different kinds of errors.

57
00:04:47,900 --> 00:04:53,500
It's like, instead of just saying the AI messed up, we can say it messed up in this specific way,

58
00:04:53,600 --> 00:04:57,060
and that tells us something. That's huge for AI development, right?

59
00:04:57,060 --> 00:05:02,300
Absolutely. It shows that LLMs aren't just these black boxes spitting out random stuff.

60
00:05:02,660 --> 00:05:08,080
They have systems, intricate ones, for how they think about truth. And those systems work

61
00:05:08,080 --> 00:05:11,240
differently depending on what kind of knowledge or reasoning is involved.

62
00:05:11,440 --> 00:05:15,760
So those systems, they might have different strengths and weaknesses, just like us. Like,

63
00:05:15,960 --> 00:05:22,320
I'm great at facts, but emotions, not so much. Sounds like LLMs could be wired the same way.

64
00:05:22,320 --> 00:05:27,220
That's a great way to put it. And here's where it gets really interesting. They didn't just categorize

65
00:05:27,220 --> 00:05:33,820
the errors. The researchers wanted to use those AI detectives, those probes, to actually

66
00:05:33,820 --> 00:05:35,180
D something about them.

67
00:05:35,400 --> 00:05:39,340
Wait. So instead of just predicting errors, they wanted to see if those probes could actually

68
00:05:39,340 --> 00:05:43,260
help them find the right answers. Like giving the LLM a multiple choice test and the probes

69
00:05:43,260 --> 00:05:44,120
looking at its answers.

70
00:05:44,200 --> 00:05:48,440
You got it. It was like that. They gave the LLM a question multiple times, got a bunch of different

71
00:05:48,440 --> 00:05:54,000
answers. Then they used their probe to choose the answer that seemed most likely to be right,

72
00:05:54,540 --> 00:05:56,700
based on the LLM's internal signals.

73
00:05:56,860 --> 00:06:03,300
So they basically gave the LLM a chance to show its work. Did it work? Did using those signals

74
00:06:03,300 --> 00:06:05,040
actually help them pick better answers?

75
00:06:05,240 --> 00:06:11,100
It did. Using the probe to pick the answer, it consistently led to better accuracy. But here's

76
00:06:11,100 --> 00:06:17,540
the kicker. The improvements were most noticeable when the LLM's internal signals and what it actually

77
00:06:17,540 --> 00:06:19,320
said didn't match up.

78
00:06:19,740 --> 00:06:23,800
You mean when the LLM seemed to know the right answer but still gave the wrong one? Those

79
00:06:23,800 --> 00:06:24,640
always blew my mind.

80
00:06:24,720 --> 00:06:25,440
Exactly those.

81
00:06:25,560 --> 00:06:25,700
Yeah.

82
00:06:25,880 --> 00:06:30,820
The biggest jumps in accuracy came when the LLM was flip-flopping between answers or even

83
00:06:30,820 --> 00:06:35,060
always giving a wrong answer, even though inside it seemed like it knew better.

84
00:06:35,200 --> 00:06:39,840
Like it's second-guessing itself or something? But why would it do that? Seems weird for an AI.

85
00:06:40,140 --> 00:06:44,680
Right. It's a great question. We don't have a perfect answer yet. But this research,

86
00:06:44,680 --> 00:06:50,940
it gives us some possibilities. One idea is maybe it's about confidence or a lack of it.

87
00:06:51,560 --> 00:06:55,960
The LLM sees these hints of the right answer, but it's just not sure enough to commit.

88
00:06:56,340 --> 00:06:59,860
So even though it has an idea of what's right, it doesn't want to bet on it. That's almost human,

89
00:06:59,980 --> 00:07:01,200
isn't it? That hesitation.

90
00:07:01,620 --> 00:07:05,700
It really is. And that's what's so cool about this research. It suggests these AI models,

91
00:07:05,940 --> 00:07:10,220
they're more complex than we think. Of course, it could be other things too. Like maybe it's the

92
00:07:10,220 --> 00:07:15,500
data the LLM was trained on. Ah, right. Those massive data sets full of text, code, all that.

93
00:07:15,840 --> 00:07:19,120
Those data sets, they've got all sorts of human biases in them, don't they?

94
00:07:19,300 --> 00:07:25,500
They do. So even if the LLM's internal signals are saying one thing, the biases from its training

95
00:07:25,500 --> 00:07:30,320
could be pushing it another way. It's like the LLM saying, well, I think this might be right,

96
00:07:30,320 --> 00:07:32,260
but everyone else seems to think it's that. So.

97
00:07:32,260 --> 00:07:38,800
It's going with the crowd, even when it has its own ideas. That's a great analogy. Shows how our

98
00:07:38,800 --> 00:07:41,440
own biases can creep into the AI we build.

99
00:07:41,940 --> 00:07:44,020
And if we don't address those biases. Yeah.

100
00:07:44,680 --> 00:07:49,340
Problems. This is why it's so important, not just to look at what the LLM says, but to understand

101
00:07:49,340 --> 00:07:54,260
how it got there, what's going on inside. If we can read those internal signals better,

102
00:07:54,460 --> 00:07:56,540
we can make AI more accurate, more trustworthy.

103
00:07:57,000 --> 00:08:02,120
It's a whole new way of looking at AI, not just did you get it right, but how did you get there?

104
00:08:02,120 --> 00:08:04,640
What were you thinking? That feels like a game changer.

105
00:08:05,200 --> 00:08:09,740
It really is. And these probing classifiers, they're still new, but they give us a way to do

106
00:08:09,740 --> 00:08:14,840
that, a way to peek inside the AI's brain and start to understand how it makes decisions.

107
00:08:15,120 --> 00:08:20,320
So we went from finding these little clues, these exact answer tokens, to figuring out

108
00:08:20,320 --> 00:08:25,200
different types of errors and maybe even helping LLM's get those answers right more often.

109
00:08:25,340 --> 00:08:27,500
Pretty amazing, right? How fast things are moving.

110
00:08:27,600 --> 00:08:30,360
It really is. But let's talk real world for a sec.

111
00:08:30,360 --> 00:08:33,900
What does this all mean for how we actually use LLM's?

112
00:08:34,040 --> 00:08:38,520
Ah, the big question. I mean, they're everywhere, right? Chatbots, search engines, the works.

113
00:08:39,200 --> 00:08:43,640
But can we really trust them, especially with important stuff like, I don't know, our health,

114
00:08:43,780 --> 00:08:44,140
our money?

115
00:08:44,240 --> 00:08:48,160
Yeah, you don't want your AI financial advisor hallucinating when you're about to retire.

116
00:08:48,320 --> 00:08:53,160
But if this research actually works, if we can make LLM's more reliable, huge potential.

117
00:08:53,160 --> 00:08:58,900
Huge. Imagine AI that's not just helpful, but you can actually depend on it. Accurate info,

118
00:08:59,100 --> 00:09:03,660
real insights, even creative solutions. No more worrying it'll go off on some weird tangent.

119
00:09:03,860 --> 00:09:08,680
Now that's exciting. But how do we get there? What's next? How do we take these research findings

120
00:09:08,680 --> 00:09:10,800
and actually make AI better?

121
00:09:10,800 --> 00:09:15,260
This paper is a great start. It gives us a whole new way to look at LLM errors,

122
00:09:15,620 --> 00:09:19,220
not just surface level, but really getting into how they work on the inside.

123
00:09:19,420 --> 00:09:22,740
No more treating them like a black box, right? We're starting to understand them.

124
00:09:22,840 --> 00:09:27,300
Exactly. And the exciting part is, can we use these internal signals,

125
00:09:27,420 --> 00:09:31,180
these little hints of confidence or doubt, to make their answers more accurate?

126
00:09:31,340 --> 00:09:34,380
So we could actually help them give the right answer even when they're unsure.

127
00:09:34,380 --> 00:09:39,680
Exactly. We might need new ways to train them, maybe even new LLM designs,

128
00:09:40,080 --> 00:09:42,980
ones that make sure their internal signals match what they tell us.

129
00:09:43,060 --> 00:09:46,340
So it's not just about more data, it's about better training,

130
00:09:46,620 --> 00:09:48,560
using what we're learning about how they think.

131
00:09:48,780 --> 00:09:53,860
Right. And as we get better at training, we can build even better probing classifiers.

132
00:09:54,100 --> 00:09:58,760
The ones they use were kind of basic. Imagine what we could do with more powerful tools

133
00:09:58,760 --> 00:10:00,500
to analyze those internal states.

134
00:10:00,500 --> 00:10:04,600
It's like, we're learning to speak their language, not just giving commands,

135
00:10:04,720 --> 00:10:07,720
but having a real dialogue, understanding how they work,

136
00:10:07,860 --> 00:10:10,160
so we can unlock their full potential.

137
00:10:10,500 --> 00:10:13,660
That's what's so cool about this field. It's not just about smart machines,

138
00:10:13,840 --> 00:10:17,840
it's about understanding intelligence itself, theirs, and ours.

139
00:10:18,320 --> 00:10:22,100
Wow. Great point to end on. This deep dive has been amazing.

140
00:10:22,240 --> 00:10:25,620
I'm really seeing LLMs in a new light. We're just at the beginning, aren't we?

141
00:10:25,700 --> 00:10:28,740
Totally. Exciting times for AI. Can't wait to see what's next.

142
00:10:28,740 --> 00:10:32,800
Me too. A huge thank you to our expert for joining us,

143
00:10:32,860 --> 00:10:35,020
for sharing all this incredible knowledge with us.

144
00:10:35,120 --> 00:10:37,260
Happy to be here. Love talking about this stuff.

145
00:10:37,400 --> 00:10:40,720
And to our listeners, thank you here for joining us on this deep dive.

146
00:10:41,000 --> 00:10:42,580
Hope you learned something new and interesting.

147
00:10:42,940 --> 00:10:45,580
Stay curious, everyone, and we'll catch you on the next one.

