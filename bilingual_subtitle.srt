1
00:00:00,000 --> 00:00:05,940
Hey everyone, ready for a deep dive? Let's talk about AI, specifically LLMs, those large language
大家好，准备好深入探讨了吗？我们来聊聊人工智能，特别是大型语言模型（LLMs）。

2
00:00:05,940 --> 00:00:10,760
models. We've got a real interesting research paper hot off the press. Title's a bit of a
模型。我们有一篇非常有趣的研究论文刚刚发布。标题有点……

3
00:00:10,760 --> 00:00:17,480
mouthful though, ready? LLMs know more than they show on the intrinsic representation of LLM
虽然有点拗口，但准备好了吗？大型语言模型（LLMs）所知道的，远超过它们在内在表示上所展示的。

4
00:00:17,480 --> 00:00:22,680
hallucinations. Kitchy, right. But it gets straight to the point, doesn't it? Why do these powerful
幻觉。听起来很俗气，对吧？但这直接切入了主题，不是吗？为什么这些强大的……

5
00:00:22,680 --> 00:00:28,200
LLMs sometimes just make stuff up? That's what we're diving into today. It's like when you ask
大型语言模型有时会胡编乱造？今天我们就来探讨这个问题。这就像当你问

6
00:00:28,200 --> 00:00:32,140
an LLM for a simple fact, you know, for work or something, and it gives you something totally
一个大型语言模型（LLM）用于简单的事实，比如工作或其他事情，但它给你的却是完全不同的东西。

7
00:00:32,140 --> 00:00:36,800
off base, you're just left there thinking, did it? Did it just make that up? Exactly. And this
偏离主题了，你只是呆在那里思考，真的？它刚刚是凭空想出来的吗？没错。还有这个。

8
00:00:36,800 --> 00:00:40,860
research team, they didn't just stop at, oops, the AI got it wrong. They actually looked inside
研究团队并没有仅仅停留在“哎呀，人工智能出错了”这一点上。他们实际上深入探究了问题。

9
00:00:40,860 --> 00:00:45,780
several different LLMs, wanted to see if there was something more going on, something about how these
几个不同的语言模型，我想看看是否还有其他的东西在发生，关于这些模型的某些特性。

10
00:00:45,780 --> 00:00:51,540
models understand or maybe misunderstand the whole idea of truth. One thing that really caught my eye
模型理解或可能误解了真理的整个概念。有一件事真的引起了我的注意。

11
00:00:51,540 --> 00:00:57,140
was this whole exact answer token thing. I know we've got some tech savvy listeners, but just in
这整个确切答案令牌的事情。我知道我们有一些技术精明的听众，但只是为了确保...

12
00:00:57,140 --> 00:01:02,940
case, you mind breaking that down for us. Sure, sure. Think of LLMs as being made of these, well,
当然可以。你能为我们详细解释一下吗？当然可以。可以把大型语言模型（LLMs）看作是由这些，嗯，组成的。

13
00:01:03,120 --> 00:01:07,980
building blocks of language. They're called tokens, kind of like how we use words, but for these
语言的构建块。它们被称为“标记”，有点像我们使用单词的方式，但对于这些来说……

14
00:01:07,980 --> 00:01:12,560
models. Now, what they found is that some tokens, when the LLM is putting together its response,
模型。现在，他们发现一些标记在大型语言模型（LLM）构建其响应时，

15
00:01:12,860 --> 00:01:19,380
they matter more when it comes to whether the model think what it's saying is true. Those are the exact
当涉及到模型是否认为它所说的是真实时，它们更为重要。那些正是确切的。

16
00:01:19,380 --> 00:01:25,340
answer tokens. So it's not just the final answer itself, but these specific tokens inside it that give
答案标记。因此，不仅仅是最终答案本身，而是其中的这些特定标记提供了。

17
00:01:25,340 --> 00:01:30,640
us clues about what the LLM is thinking, right? Like, is it sure of itself or is it about to go off the
这给我们提供了关于大型语言模型（LLM）思维的线索，对吧？比如，它是否对自己很有信心，或者它是否快要偏离轨道了。

18
00:01:30,640 --> 00:01:35,960
rails? Exactly. And that's a big deal because you see, a lot of the time people just looked at the
轨道？没错。这很重要，因为你看，很多时候人们只是看到了……

19
00:01:35,960 --> 00:01:42,680
final output, the last token to understand errors. This research says, hold on, there's more to it.
最终输出，最后一个标记用于理解错误。这项研究表明，等等，还有更多内容。

20
00:01:43,060 --> 00:01:47,900
These exact answer tokens, often right in the middle of the response, they're like a much stronger
这些确切的答案标记，通常位于回应的正中间，它们就像是更强大的。

21
00:01:47,900 --> 00:01:52,780
signal about how confident the model is. It's almost like the LLM is like emphasizing,
信号表明模型的自信程度。几乎就像大型语言模型在强调一样，

22
00:01:52,780 --> 00:01:57,620
maybe even second guessing, the most important part of its answer. Makes you wonder if we could
也许甚至会对其答案中最重要的部分产生怀疑。这让人想知道我们是否能够……

23
00:01:57,620 --> 00:02:01,160
use this to predict when an LLM is about to have one of those hallucination moments.
用这个来预测一个大型语言模型（LLM）何时即将出现幻觉时刻。

24
00:02:01,500 --> 00:02:05,400
That's what the researchers wanted to know. They actually built these little AI detectives,
这正是研究人员想要了解的。他们实际上构建了这些小型人工智能侦探，

25
00:02:05,560 --> 00:02:12,260
probing classifiers, to look at the LLM's internal signals at the exact moment it's processing these
探测分类器，以便在大型语言模型处理这些内容的确切时刻查看其内部信号。

26
00:02:12,260 --> 00:02:18,400
exact answer tokens, like trying to read the AI's mind. Okay, hold on. So they built AIs to spy on
确切的答案令牌，就像试图读懂人工智能的心思。好的，等一下。他们构建了人工智能来进行监视。

27
00:02:18,400 --> 00:02:23,740
other AIs. Wild. But did it work? Could they tell if an LLM was about to give a wrong answer
其他人工智能。疯狂。但这有效吗？他们能否判断一个大型语言模型是否即将给出错误答案？

28
00:02:23,740 --> 00:02:28,300
just by looking at those tokens? It's pretty amazing. But yeah, they were surprisingly accurate.
仅仅通过查看那些代币？这真是令人惊讶。不过，是的，它们的准确性出乎意料。

29
00:02:28,720 --> 00:02:32,380
Way better at predicting those errors than older methods that focused on other things.
比起那些关注其他方面的旧方法，这种方法在预测这些错误方面要好得多。

30
00:02:32,520 --> 00:02:36,720
So we're talking about a potential breakthrough here, right? By looking at how much emphasis an
所以我们在谈论一个潜在的突破，对吧？通过观察一个……的重视程度。

31
00:02:36,720 --> 00:02:42,240
LLM puts on certain words, we might be able to tell if we should trust what it's saying. That's huge
LLM使用某些词汇，我们可能能够判断是否应该信任它所说的内容。这非常重要。

32
00:02:42,240 --> 00:02:47,060
for making AI more reliable. It really is. But there's always a but, isn't there? They didn't
为了让人工智能变得更加可靠。确实是这样。但总是有一个“但是”，不是吗？他们没有……

33
00:02:47,060 --> 00:02:51,520
stop there. The researchers wanted to know if these AI detectives, these probing classifiers,
停在那里。研究人员想知道这些人工智能侦探，这些探测分类器，

34
00:02:51,620 --> 00:02:56,540
could generalize. Could they spot errors across different types of tasks? So if they trained a
可以进行概括。他们能否在不同类型的任务中发现错误？所以如果他们训练一个……

35
00:02:56,540 --> 00:03:03,280
probe on, say, trivia questions, would it also catch errors in, I don't know, analyzing a movie review?
如果对琐事问题进行探讨，它是否也会发现分析电影评论时的错误呢？

36
00:03:03,660 --> 00:03:08,320
Exactly. And this is where things get even more interesting. While the probes did show some
没错。这就是事情变得更加有趣的地方。虽然探测器确实显示了一些……

37
00:03:08,320 --> 00:03:15,280
ability to transfer their error spotting skills, it wasn't a perfect system. It seems like different
将他们的错误发现技能转移的能力并不是一个完美的系统。看起来不同的

38
00:03:15,280 --> 00:03:21,160
AI skills, like knowing a fact versus understanding how someone feels, those might rely on different
AI技能，比如知道一个事实与理解某人的感受，这些可能依赖于不同的能力。

39
00:03:21,160 --> 00:03:27,820
mechanisms for figuring out truth. So no single lie detector for LLMs, then? Makes sense, I guess.
揭示真相的机制。那么，对于大型语言模型（LLMs）来说，没有单一的谎言探测器吗？我想这很有道理。

40
00:03:27,940 --> 00:03:32,400
Our brains are complicated, so why wouldn't these models be the same? But if that's the case,
我们的脑袋很复杂，那这些模型为什么不会一样呢？但如果是这样的话，

41
00:03:32,480 --> 00:03:36,640
how do we even begin to understand these different errors, let alone fix them?
我们该如何开始理解这些不同的错误，更不用说修复它们了？

42
00:03:36,640 --> 00:03:40,960
Well, the researchers, they really dug into this. They focused on one type of task,
好吧，研究人员对此进行了深入研究。他们专注于一种类型的任务，

43
00:03:41,660 --> 00:03:47,140
answering trivia questions. But instead of just right or wrong, they came up with this whole system
回答琐事问题。但他们并不仅仅是判断对错，而是想出了整个系统。

44
00:03:47,140 --> 00:03:51,240
to categorize errors based on how often the LLM messed up the same question.
根据大型语言模型（LLM）在同一问题上出错的频率对错误进行分类。

45
00:03:51,780 --> 00:03:55,720
So they were looking for patterns, trying to understand the AI's thought process. Like,
所以他们在寻找模式，试图理解人工智能的思维过程。比如，

46
00:03:55,800 --> 00:03:58,340
okay, sometimes you get this right, sometimes wrong. What's the deal?
好的，有时候你做对了，有时候又做错了。这是怎么回事？

47
00:03:58,700 --> 00:04:03,060
Exactly. And they found some interesting things. For example, sometimes the LLM would mostly get an
没错。他们发现了一些有趣的事情。例如，有时大型语言模型（LLM）会主要得到一个……

48
00:04:03,060 --> 00:04:09,040
answer wrong, but then, bam, nail it out of nowhere. That little difference, always wrong versus
回答错误，但接着，砰，一下子就答对了。那一点小差别，总是错误与……

49
00:04:09,040 --> 00:04:13,180
sometimes right, could mean completely different things are causing those errors.
有时候，正确的判断可能意味着导致这些错误的原因完全不同。

50
00:04:13,180 --> 00:04:19,600
That IS wild. Like, the LLM knows the answer somewhere in there, but can't always access it.
这真是太疯狂了。就像大型语言模型在某个地方知道答案，但并不总是能够访问到它。

51
00:04:19,660 --> 00:04:25,140
It's like when you're taking a test and you know you studied it, but it's just not there. But could the
这就像你在考试时，明明知道自己复习过，但就是想不起来。不过，可能……

52
00:04:25,140 --> 00:04:30,720
researchers predict what kind of mistake the LLM would make just by looking at those signals inside?
研究人员仅通过观察内部信号，预测大型语言模型（LLM）会犯什么样的错误吗？

53
00:04:30,720 --> 00:04:33,720
That just seems next level. You're not going to believe this. They gave it a shot.
这看起来真是一个新高度。你不会相信的。他们尝试了一下。

54
00:04:33,860 --> 00:04:38,360
Yeah. And the results were pretty impressive. Those error types, those different ways an LLM
好的。结果相当令人印象深刻。这些错误类型，以及LLM的不同表现方式。

55
00:04:38,360 --> 00:04:43,400
can trip up, they are somewhat predictable just by, you know, looking at the model's internal signals.
可以出错，但通过观察模型的内部信号，它们在某种程度上是可以预测的。

56
00:04:43,640 --> 00:04:47,720
So we're not just spotting errors anymore. We're figuring out the different kinds of errors.
所以我们不仅仅是在发现错误。我们还在识别不同类型的错误。

57
00:04:47,900 --> 00:04:53,500
It's like, instead of just saying the AI messed up, we can say it messed up in this specific way,
这就像是，我们不仅仅说人工智能出错了，而是可以具体指出它在某个特定方面出错了。

58
00:04:53,600 --> 00:04:57,060
and that tells us something. That's huge for AI development, right?
这告诉我们一些事情。这对人工智能的发展来说是非常重要的，对吧？

59
00:04:57,060 --> 00:05:02,300
Absolutely. It shows that LLMs aren't just these black boxes spitting out random stuff.
当然。这表明大型语言模型（LLMs）并不仅仅是这些随机输出内容的黑箱。

60
00:05:02,660 --> 00:05:08,080
They have systems, intricate ones, for how they think about truth. And those systems work
他们有复杂的系统来思考真理。这些系统是有效的。

61
00:05:08,080 --> 00:05:11,240
differently depending on what kind of knowledge or reasoning is involved.
根据所涉及的知识或推理的不同，结果会有所不同。

62
00:05:11,440 --> 00:05:15,760
So those systems, they might have different strengths and weaknesses, just like us. Like,
所以这些系统可能有不同的优缺点，就像我们一样。比如，

63
00:05:15,960 --> 00:05:22,320
I'm great at facts, but emotions, not so much. Sounds like LLMs could be wired the same way.
我擅长处理事实，但情感方面就不太行了。听起来大型语言模型（LLMs）也可能是这样。

64
00:05:22,320 --> 00:05:27,220
That's a great way to put it. And here's where it gets really interesting. They didn't just categorize
这真是个很好的表达方式。接下来就变得非常有趣了。他们不仅仅是对……进行了分类。

65
00:05:27,220 --> 00:05:33,820
the errors. The researchers wanted to use those AI detectives, those probes, to actually
错误。研究人员希望利用这些人工智能侦探，这些探测器，来实际地

66
00:05:33,820 --> 00:05:35,180
D something about them.
关于他们的某些事情。

67
00:05:35,400 --> 00:05:39,340
Wait. So instead of just predicting errors, they wanted to see if those probes could actually
等一下。所以他们不仅仅是想预测错误，而是想看看那些探针是否真的能够……

68
00:05:39,340 --> 00:05:43,260
help them find the right answers. Like giving the LLM a multiple choice test and the probes
帮助他们找到正确的答案。就像给大型语言模型（LLM）一个选择题测试和探测器一样。

69
00:05:43,260 --> 00:05:44,120
looking at its answers.
查看它的答案。

70
00:05:44,200 --> 00:05:48,440
You got it. It was like that. They gave the LLM a question multiple times, got a bunch of different
你说得对。就是那样。他们多次给大型语言模型（LLM）提问，得到了许多不同的答案。

71
00:05:48,440 --> 00:05:54,000
answers. Then they used their probe to choose the answer that seemed most likely to be right,
答案。然后他们使用探测器选择看起来最有可能正确的答案，

72
00:05:54,540 --> 00:05:56,700
based on the LLM's internal signals.
基于大型语言模型的内部信号。

73
00:05:56,860 --> 00:06:03,300
So they basically gave the LLM a chance to show its work. Did it work? Did using those signals
所以他们基本上给了大型语言模型一个展示其工作的机会。它有效吗？使用这些信号是否有用？

74
00:06:03,300 --> 00:06:05,040
actually help them pick better answers?
实际上帮助他们选择更好的答案吗？

75
00:06:05,240 --> 00:06:11,100
It did. Using the probe to pick the answer, it consistently led to better accuracy. But here's
它确实如此。使用探针来选择答案，结果始终能提高准确性。但这里有一个问题。

76
00:06:11,100 --> 00:06:17,540
the kicker. The improvements were most noticeable when the LLM's internal signals and what it actually
关键在于。当大型语言模型（LLM）的内部信号与其实际输出之间的差异最为明显时，改进效果尤为显著。

77
00:06:17,540 --> 00:06:19,320
said didn't match up.
说的不一致。

78
00:06:19,740 --> 00:06:23,800
You mean when the LLM seemed to know the right answer but still gave the wrong one? Those
你是说当大型语言模型似乎知道正确答案但仍然给出了错误答案的时候吗？那些

79
00:06:23,800 --> 00:06:24,640
always blew my mind.
总是让我感到震惊。

80
00:06:24,720 --> 00:06:25,440
Exactly those.
正是那些。

81
00:06:25,560 --> 00:06:25,700
Yeah.
好的。

82
00:06:25,880 --> 00:06:30,820
The biggest jumps in accuracy came when the LLM was flip-flopping between answers or even
准确性最大的提升出现在大型语言模型（LLM）在答案之间反复切换时，甚至是……

83
00:06:30,820 --> 00:06:35,060
always giving a wrong answer, even though inside it seemed like it knew better.
总是给出错误的答案，尽管内心似乎知道得更好。

84
00:06:35,200 --> 00:06:39,840
Like it's second-guessing itself or something? But why would it do that? Seems weird for an AI.
就像它在自我怀疑一样？但它为什么会这样做呢？对一个人工智能来说，这似乎很奇怪。

85
00:06:40,140 --> 00:06:44,680
Right. It's a great question. We don't have a perfect answer yet. But this research,
好的。这是一个很好的问题。我们还没有一个完美的答案。但这项研究，

86
00:06:44,680 --> 00:06:50,940
it gives us some possibilities. One idea is maybe it's about confidence or a lack of it.
这给了我们一些可能性。一个想法是，也许这与自信或缺乏自信有关。

87
00:06:51,560 --> 00:06:55,960
The LLM sees these hints of the right answer, but it's just not sure enough to commit.
大型语言模型（LLM）看到了正确答案的这些线索，但它并不够确定，因此无法做出决定。

88
00:06:56,340 --> 00:06:59,860
So even though it has an idea of what's right, it doesn't want to bet on it. That's almost human,
所以即使它对什么是正确的有一定的认识，它也不想对此下赌注。这几乎是人类的特征。

89
00:06:59,980 --> 00:07:01,200
isn't it? That hesitation.
不是吗？那种犹豫。

90
00:07:01,620 --> 00:07:05,700
It really is. And that's what's so cool about this research. It suggests these AI models,
确实如此。这就是这项研究如此酷的地方。它暗示了这些人工智能模型，

91
00:07:05,940 --> 00:07:10,220
they're more complex than we think. Of course, it could be other things too. Like maybe it's the
它们比我们想象的更复杂。当然，也可能是其他原因。比如说，也许是因为……

92
00:07:10,220 --> 00:07:15,500
data the LLM was trained on. Ah, right. Those massive data sets full of text, code, all that.
LLM训练所用的数据。啊，对了。那些包含文本、代码等的大型数据集。

93
00:07:15,840 --> 00:07:19,120
Those data sets, they've got all sorts of human biases in them, don't they?
这些数据集里包含了各种人类偏见，对吧？

94
00:07:19,300 --> 00:07:25,500
They do. So even if the LLM's internal signals are saying one thing, the biases from its training
它们确实如此。因此，即使大型语言模型的内部信号在传达某种信息，其训练中的偏见也会影响结果。

95
00:07:25,500 --> 00:07:30,320
could be pushing it another way. It's like the LLM saying, well, I think this might be right,
可能会以另一种方式推动。这就像大型语言模型在说：“嗯，我认为这可能是正确的。”

96
00:07:30,320 --> 00:07:32,260
but everyone else seems to think it's that. So.
但其他人似乎都认为是那样的。所以。

97
00:07:32,260 --> 00:07:38,800
It's going with the crowd, even when it has its own ideas. That's a great analogy. Shows how our
这与人群保持一致，即使它有自己的想法。这是一个很好的类比。显示了我们的……

98
00:07:38,800 --> 00:07:41,440
own biases can creep into the AI we build.
我们构建的人工智能中可能会渗入我们自身的偏见。

99
00:07:41,940 --> 00:07:44,020
And if we don't address those biases. Yeah.
如果我们不解决这些偏见的话。是的。

100
00:07:44,680 --> 00:07:49,340
Problems. This is why it's so important, not just to look at what the LLM says, but to understand
问题。这就是为什么不仅要关注大型语言模型（LLM）所说的内容，还要理解其背后的原因是如此重要。

101
00:07:49,340 --> 00:07:54,260
how it got there, what's going on inside. If we can read those internal signals better,
它是如何到达那里的，里面发生了什么。如果我们能更好地解读这些内部信号，

102
00:07:54,460 --> 00:07:56,540
we can make AI more accurate, more trustworthy.
我们可以让人工智能变得更加准确，更加可信。

103
00:07:57,000 --> 00:08:02,120
It's a whole new way of looking at AI, not just did you get it right, but how did you get there?
这是一种全新的看待人工智能的方式，不仅仅是你是否做对了，而是你是如何做到的？

104
00:08:02,120 --> 00:08:04,640
What were you thinking? That feels like a game changer.
你在想什么？这感觉像是一个游戏规则的改变者。

105
00:08:05,200 --> 00:08:09,740
It really is. And these probing classifiers, they're still new, but they give us a way to do
确实如此。这些探测分类器仍然是新的，但它们为我们提供了一种方法来做……

106
00:08:09,740 --> 00:08:14,840
that, a way to peek inside the AI's brain and start to understand how it makes decisions.
这是一种窥探人工智能大脑的方式，开始理解它是如何做出决策的。

107
00:08:15,120 --> 00:08:20,320
So we went from finding these little clues, these exact answer tokens, to figuring out
所以我们从寻找这些小线索，这些确切的答案标记，转变为弄清楚

108
00:08:20,320 --> 00:08:25,200
different types of errors and maybe even helping LLM's get those answers right more often.
不同类型的错误，甚至可能帮助大型语言模型（LLM）更频繁地得到正确答案。

109
00:08:25,340 --> 00:08:27,500
Pretty amazing, right? How fast things are moving.
相当惊人，对吧？事情发展得多么迅速。

110
00:08:27,600 --> 00:08:30,360
It really is. But let's talk real world for a sec.
确实是这样。不过让我们先谈谈现实世界的情况。

111
00:08:30,360 --> 00:08:33,900
What does this all mean for how we actually use LLM's?
这对我们实际使用大型语言模型（LLM）意味着什么？

112
00:08:34,040 --> 00:08:38,520
Ah, the big question. I mean, they're everywhere, right? Chatbots, search engines, the works.
啊，那个大问题。我是说，它们无处不在，对吧？聊天机器人、搜索引擎，各种各样的。

113
00:08:39,200 --> 00:08:43,640
But can we really trust them, especially with important stuff like, I don't know, our health,
但我们真的能信任他们吗，尤其是在一些重要的事情上，比如，我不知道，我们的健康。

114
00:08:43,780 --> 00:08:44,140
our money?
我们的钱？

115
00:08:44,240 --> 00:08:48,160
Yeah, you don't want your AI financial advisor hallucinating when you're about to retire.
是的，当你即将退休时，你可不希望你的人工智能财务顾问出现幻觉。

116
00:08:48,320 --> 00:08:53,160
But if this research actually works, if we can make LLM's more reliable, huge potential.
但如果这项研究真的有效，如果我们能让大型语言模型（LLM）更可靠，那将有巨大的潜力。

117
00:08:53,160 --> 00:08:58,900
Huge. Imagine AI that's not just helpful, but you can actually depend on it. Accurate info,
巨大的。想象一下，人工智能不仅仅是有用的，而且你实际上可以依赖它。准确的信息，

118
00:08:59,100 --> 00:09:03,660
real insights, even creative solutions. No more worrying it'll go off on some weird tangent.
真实的洞察，甚至是创造性的解决方案。再也不用担心它会偏离主题。

119
00:09:03,860 --> 00:09:08,680
Now that's exciting. But how do we get there? What's next? How do we take these research findings
这真令人兴奋。但我们如何到达那里呢？接下来是什么？我们如何利用这些研究发现？

120
00:09:08,680 --> 00:09:10,800
and actually make AI better?
并实际上使人工智能变得更好？

121
00:09:10,800 --> 00:09:15,260
This paper is a great start. It gives us a whole new way to look at LLM errors,
这篇论文是一个很好的开端。它为我们提供了一种全新的视角来审视大型语言模型的错误。

122
00:09:15,620 --> 00:09:19,220
not just surface level, but really getting into how they work on the inside.
不仅仅是表面层面，而是深入了解它们内部的运作方式。

123
00:09:19,420 --> 00:09:22,740
No more treating them like a black box, right? We're starting to understand them.
不再把它们当作黑箱对待，对吧？我们开始理解它们了。

124
00:09:22,840 --> 00:09:27,300
Exactly. And the exciting part is, can we use these internal signals,
没错。令人兴奋的是，我们能否利用这些内部信号，

125
00:09:27,420 --> 00:09:31,180
these little hints of confidence or doubt, to make their answers more accurate?
这些小小的自信或怀疑的暗示，能否使他们的回答更准确？

126
00:09:31,340 --> 00:09:34,380
So we could actually help them give the right answer even when they're unsure.
所以我们实际上可以帮助他们在不确定的时候也能给出正确的答案。

127
00:09:34,380 --> 00:09:39,680
Exactly. We might need new ways to train them, maybe even new LLM designs,
没错。我们可能需要新的方法来训练它们，甚至可能需要新的大型语言模型设计。

128
00:09:40,080 --> 00:09:42,980
ones that make sure their internal signals match what they tell us.
确保他们内部信号与他们告诉我们的内容相符的那些人。

129
00:09:43,060 --> 00:09:46,340
So it's not just about more data, it's about better training,
所以这不仅仅是关于更多的数据，而是关于更好的训练。

130
00:09:46,620 --> 00:09:48,560
using what we're learning about how they think.
利用我们所学到的关于他们思维方式的知识。

131
00:09:48,780 --> 00:09:53,860
Right. And as we get better at training, we can build even better probing classifiers.
没错。随着我们在训练方面的进步，我们可以构建更优秀的探测分类器。

132
00:09:54,100 --> 00:09:58,760
The ones they use were kind of basic. Imagine what we could do with more powerful tools
他们使用的工具有点基础。想象一下，如果我们有更强大的工具可以做些什么。

133
00:09:58,760 --> 00:10:00,500
to analyze those internal states.
分析这些内部状态。

134
00:10:00,500 --> 00:10:04,600
It's like, we're learning to speak their language, not just giving commands,
这就像是，我们在学习用他们的语言交流，而不仅仅是发号施令。

135
00:10:04,720 --> 00:10:07,720
but having a real dialogue, understanding how they work,
但进行真正的对话，了解他们是如何运作的，

136
00:10:07,860 --> 00:10:10,160
so we can unlock their full potential.
这样我们就能释放他们的全部潜力。

137
00:10:10,500 --> 00:10:13,660
That's what's so cool about this field. It's not just about smart machines,
这就是这个领域如此酷的地方。它不仅仅是关于智能机器，

138
00:10:13,840 --> 00:10:17,840
it's about understanding intelligence itself, theirs, and ours.
这关乎理解智力本身，包括他们的和我们的智力。

139
00:10:18,320 --> 00:10:22,100
Wow. Great point to end on. This deep dive has been amazing.
哇，真是个很好的总结。这次深入探讨太棒了。

140
00:10:22,240 --> 00:10:25,620
I'm really seeing LLMs in a new light. We're just at the beginning, aren't we?
我真的对大型语言模型（LLMs）有了新的认识。我们才刚刚开始，不是吗？

141
00:10:25,700 --> 00:10:28,740
Totally. Exciting times for AI. Can't wait to see what's next.
完全同意。人工智能的时代令人兴奋。迫不及待想看看接下来会发生什么。

142
00:10:28,740 --> 00:10:32,800
Me too. A huge thank you to our expert for joining us,
我也是。非常感谢我们的专家加入我们。

143
00:10:32,860 --> 00:10:35,020
for sharing all this incredible knowledge with us.
感谢您与我们分享这些令人难以置信的知识。

144
00:10:35,120 --> 00:10:37,260
Happy to be here. Love talking about this stuff.
很高兴来到这里。喜欢谈论这些事情。

145
00:10:37,400 --> 00:10:40,720
And to our listeners, thank you here for joining us on this deep dive.
感谢我们的听众，感谢你们加入我们这次深入探讨。

146
00:10:41,000 --> 00:10:42,580
Hope you learned something new and interesting.
希望你学到了一些新鲜有趣的东西。

147
00:10:42,940 --> 00:10:45,580
Stay curious, everyone, and we'll catch you on the next one.
保持好奇，大家，我们下次再见。

